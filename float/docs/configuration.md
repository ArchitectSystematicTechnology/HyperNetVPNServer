Configuration
=============

The toolkit configuration is split into two parts, the *service
metadata*, containing definitions of the known services, and a *host
inventory*, with information about each host. A number of global
variables are also required, to customize the results for your
application.

All files are YAML-encoded and should usually have a *.yml* extension.

# Main configuration

The service and host configuration is then turned into an Ansible
inventory and a bunch of Ansible variables by an [inventory
plugin](../plugins/inventory/float.py). The toolkit is configured with
a single main configuration file, pointing at the other resources,
which will be used as the *inventory* in the Ansible command-line
tool.

A minimal example of a working config:

```yaml
services_file: services.yml
passwords_file: passwords.yml
hosts_file: hosts.yml
credentials_dir: credentials/
plugin: float
```

The attributes supported are:

`services_file` points at the location of the file containing the
service metadata definition.

`hosts_file` points at the location of the hosts inventory.

`passwords_file` points at the configuration of the application
credentials (passwords).

`credentials_dir` points at the directory where autogenerated
service-level credentials (PKI-related) will be stored. This is often
a separate git repository.

`plugin` must always have the literal value *float*.

Relative paths in *float* configuration files are interpreted as
relative to the configuration file being evaluated. Among other
things, this results in the possibility of using relative paths in
*include* directives.

# Inventory (*hosts.yml*)

The inventory file defines *hosts* and *groups*, and custom variables
associated with those. It's just another way of defining an Ansible
inventory that is easy for us to inspect programatically.

The groups defined here can be used in your own Ansible playbook, but
most importantly are used in *services.yml* to make scheduling
decisions (see [Scheduling](#scheduling) below).

The inventory file must contain a dictionary encoded in YAML
format. The top-level attributes supported are:

`hosts` must contain a dictionary of *name*: *attributes* pairs
defining all the hosts in the inventory;

`group_vars` can contain a dictionary of *group\_name*: *attributes*
pairs that define group variables.

## Host variables

Variables can be Ansible variables: SSH parameters, etc., usually with
an *ansible_* prefix. But some host variables are special:

`ip` (mandatory) is the IPv4 address of this host that other hosts
should use to reach it

`ip6` (optional) is the IPv6 version of the above

`public_ip` (optional) is the IPv4 address that will be advertised in
the public-facing DNS zones, if unset it defaults to `ip`

`public_ip6` (optional) is the IPv6 version of the above

`ip_<name>` (optional) defines the IPv4 address for this host on the
[overlay network](service_mesh.md#network-overlays) named *name*

`groups` (optional) is a list of groups that this host should be a
member of.

## Example

An example of a valid inventory file (for a hypotetic Vagrant
environment):

```yaml
hosts:
  host1:
    ansible_host: 192.168.10.10
    ip: 192.168.10.10
    groups: [vagrant]
  host2:
    ansible_host: 192.168.10.11
    ip: 192.168.10.11
    groups: [vagrant]
group_vars:
  vagrant:
    ansible_become: true
    ansible_user: vagrant
    ansible_ssh_private_key_file: "~/.vagrant.d/insecure_private_key"
```

This defines two hosts (*host1* and *host2*), both part of the
*vagrant* group. Some Ansible variables are defined, both at the host
and the group level, to set Vagrant-specific connection parameters.


# Service metadata (*services.yml*)

The service metadata file (*services.yml*) is a dictionary encoded in
YAML format, where keys are service names and values contain the
associated metadata. This file is consumed by the static service
scheduler that assigns services to hosts, and by the Ansible
automation in order to define configuration variables.

Metadata for services that are part of the core infrastructure ships
embedded with the software, so when writing your own `services.yml`
file, you only need to add your services to it. You should include the
*services.yml.default* file shipped with the float source, which
defines all the built-in services:

```yaml
include:
  - "/path/to/float/services.yml.default"
```

The `include` directive is special: it does not define a service, but
it expects a list of other files to include, containing service
definitions. These are evaluated before the current file, and the
results are merged, so it is possible to override parts of an included
service definition.

Service metadata is encoded as a dictionary of *service name*:
*service attributes* pairs, each defining a separate
service. Supported attributes can be grouped in categories for
clarity.

Note that it is possible to override service metadata attributes
defined in an included file, for instance consider the following
two files:

* `base.yml`

```yaml
foo:
  scheduling_group: foo-hosts
  num_instances: 1
```

* `services.yml`

```yaml
include:
  - "base.yml"
foo:
  num_instances: 2
```

The resulting *foo* service will have *num_instances* set to 2. Note
that the merging algorithm is trivial and it's not possible to extend
or modify a list (it can only be overridden).

### Scheduling

Attributes that affect how a service is scheduled on the available
hosts.

`scheduling_group`: Only schedule the service on hosts of the
specified host group. By default, schedule on all hosts. If one needs
to specify multiple groups, use the plural `scheduling_groups` variant
of this attribute.

`num_instances`: Run a limited number of instances of the service
(selected among the hosts identified by the `scheduling_group`). By
default this is set to `all`, which will run an instance on every
host.

`master_election`: If true, pick one of the hosts as master/leader
(default is false).

### Credentials

For general information about service-level credentials, check out the
[Mutual Service
Authentication](service_mesh.md#mutual-service-authentication) section
of the docs.

Float creates a UNIX group to access each set of service-level
credentials separately, named *credentials_name*-credentials.

`service_credentials`: A list of dictionaries, one for each service
credential that should be generated for this service.

Each credential object supports the following attributes:

`name` (mandatory): Name for this set of credentials, usually the same
as the service name. Certificates will be stored in a directory with
this name below `/etc/credentials/x509`.

`enable_client`: Whether to generate a client certificate (true by
default).

`client_cert_mode`: Key usage bits to set on the client
certificate. One of *client*, *server*, or *both*, the default is
*client*.

`enable_server`: Whether to generate a server certificate (true by
default).

`server_cert_mode`: Key usage bits to set on the server
certificate. One of *client*, *server* or *both*, the default is
*server*.

`extra_san`: Additional DNS domains to add as subjectAltName fields in
the generated server certificate. This should be a list. The internal
domain name will be appended to all entries.

### Monitoring

If monitoring endpoints are defined for a service, the monitoring
infrastructure (Prometheus-based) will automatically scrape them.

For more details on the monitoring infrastructure check out the
[README of the prometheus Ansible
role](../roles/prometheus/README.md).

`monitoring_endpoints`: List of monitoring endpoints exported by the
service.

Each entry in the monitoring endpoints list can have the following
attributes:

`job_name`: Job name in Prometheus, defaults to the service name.

`type` (deprecated): Selects the service discovery mechanism used by
Prometheus to find the service endpoints. This can only have the value
*static*, which is also the default.

`port`: Port where the `/metrics` endpoint is exported.

`scheme`: HTTP scheme for the service endpoint. The default is *https*.

`metrics_path`: Path for metrics if different from the default of `/metrics`.

### Global traffic routing

Services can define *public* HTTP and TCP endpoints, that will be
exported as subdomains of the public domain name by the public
front-ends.

Normally DNS entries and SSL certificates are created for all public
endpoints. This automation can be switched off by setting the
`skip_acme` or `skip_dns` attributes to *true* (if for some reason you
need to customize these parts manually).

#### HTTP

`public_endpoints`: List of HTTP endpoints exported by the service that
should be made available to end users via the service HTTP router.

All public_endpoints are exposed to users under their own subdomain
(unless *domains* is defined), over HTTPS, on the default HTTPS port
(443).

Entries in the public endpoints list can have the following attributes:

`name`: Public name of the service. This can be different from the
service name, for instance you might want to export the internal
*prometheus* service as *monitoring* under the user-facing external
domain name. This name will be prepended to each one of the domains
listed in *domain_public* to obtain the public FQDNs to
use. Alternatively, you can define one or more *domains*.

`domains`: List of fully qualified server names for this endpoint, in
alternative or in addition to specifying a short *name*.

`port`: Port where the service is running.

`scheme`: HTTP scheme for the service endpoint. The default is *https*.

`autoconfig`: If False, disable generation of the NGINX configuration
for this host - it is assumed that some other automation will do it.

`extra_nginx_config`: Additional NGINX directives that should be
included (at the *server* level) in the generated configuration.

#### HTTP (All domains)

`horizontal_endpoints`: List of HTTP endpoints exported by the
service, that should be made available to end users on all domains
served by the infrastructure - normally used for */.well-known/* paths
and such.

Entries can have the following attributes:

`path`: Path that should be routed to the service,
e.g. */.well-known/something*. It should not end with a slash.

`port`: Port where the service is running.

`scheme`: HTTP scheme for the service endpoint. The default is *http*.

#### TCP

`public_tcp_endpoints`: List of TCP endpoints to be publicly exported
by the service.

Entries in the public TCP endpoints list can have the following
attributes, all required:

`name`: Name of the endpoint.

`port`: Port where the service is running. Also the port that will be
publicly exported (at least in the current implementation), which
unfortunately means that the service itself shouldn't be running on
*frontend* nodes.

#### Other endpoints

Other endpoints are used when the service runs their own proxies,
but we'd still like *float* to take care of generating DNS entries and
SSL certificates.

`public_other_endpoints`: List of other endpoints to be publicly
exported by the service.

Entries in the endpoints list can have the following attributes, all
required:

`name`: Name of the endpoint.

### Containers

Services can either be configured via an Ansible role, or by deploying
Docker containers (or both). Definitions for these containers are part
of the service metadata.

Containers will be registered on the DNS-based dynamic service
discovery mechanism as *service*-*container*, while the controlling
systemd service will be called docker-*service*-*container*.

Normally float will create a dedicated system user/group pair for the
containers in each service, named "docker-*service*". All containers
in the service will run as that user, unless they have the *root*
attribute set to true. These users can be used to lock down
permissions on volumes mounted in the container (usually via the
associated Ansible role). The service container user is also a member
of the service credentials groups associated with the service.

`containers`: List of containerized instances that make up the
service (for container-based services).

Each entry in this list represents a containerized image to be
run. Supported attributes include:

`name`: Name of the container. It is possible to have containers with
the same name in different services.

`image`: Which Docker image to run.

`port`: Port exposed by the Docker image. It will be exposed on the
host network interface.

`ports` (in alternative to `port`): List of ports exposed by the
Docker image. Use when you need a list, in place of `port`.

`docker_options`: Additional options to be passed to the `docker run`
command.

`args`: Arguments to be passed to the container entry point.

`volumes`: Map of *source*:*target* paths to bind-mount in the
container.
If the *source* is literally `tmpfs`, we will mount a tmpfs
filesystem (with a default size of 64MB) instead.

`root` (boolean, default: false): if set, the container will run as
root, instead of the dedicated service-level user. Enabling this
option automatically sets *drop_capabilities* to false.

`drop_capabilities` (boolean, default: true): if set, causes Docker to
drop all capabilities for this container. Otherwise, the capability
set will be controlled by systemd.

#### Naming scheme for container-based services

In order to avoid having confusing names for Docker containers and
systemd units, it's best to define upfront a naming scheme for
container-based services, or the results will get chaotic really fast.

Some generic guidelines that may be useful:

* keep service names short and meaningful (e.g. *web* is probably not
  a good name for a service);
* name containers based on the exposed protocol (*http*, *sql* etc),
  to emphasize what they do over what they are.

### Non-container services

Infrastructure components, and services with specific requirements
that make containers unsuitable, can run using Debian packages and
dedicated Ansible roles. In this case though, the service definition
should specify the following attribute:

`systemd_services`: List of systemd service units that are associated
with this service. Setting this attribute does nothing (the dedicated
Ansible role is expected to install the package fully, including
setting up systemd), except providing grouping information to help
*float* generate monitoring dashboards.

This allows *float* to turn down those services on hosts where they
should not be running.

### Additional service ports

Ports declared in *public_endpoints* and *containers* are
automatically allowed internal traffic on the host firewall. But often
internal services may want to expose ports to other internal clients:
these ports should be declared in the service definition so that
*float* can configure the firewall accordingly.

`ports`: List of ports exposed by the service over the private
networks (traffic will be allowed for both UDP and TCP).

Allowing access to *public* ports requires deploying a customized
firewall configuration snippet via Ansible.

### Datasets

Datasets allow you to describe data that is attached to a service:
this information will be used to automatically configure the backup
system. A dataset is either a local filesystem path, or something that
can be dumped/loaded via a pipe. It is associated with every instance
of the service, so it usually identifies local data. This assumes a
partitioned service by default. But master-elected services can use
the *on_master_only* option to make backups of global, replicated
datasets only once (on the service master host).

Datasets are listed using this service attribute:

`datasets`: List of dataset definitions.

Each dataset definition can have the following attributes:

`name`: Name of the dataset (mandatory).

`schedule`: A schedule on which to run the backup job for this
dataset. This can be either a time specification in the standard
*cron* format, or the special syntax

  > `@random_every` *interval*

which schedules the backup job at a random offset for each host within
the specified interval. Intervals can be written in a human-friendly
syntax like *7d* or *12h*. This is the fundamental mechanism for
spreading the load of the different hosts on the backup server without
central coordination.

`path`: Local (on each host) filesystem path that contains the
dataset. This selects the *file* type for the dataset, and is
alternative to the *backup_command* / *restore_command* attributes.

`backup_command` / `restore_command`: Shell commands for backing up
datasets via stdin/stdout pipes. If these attributes are specified,
the source is of *pipe* type. They are alternative to the *path*
attribute.

`on_master_only`: If this attribute is true, and the service's
*master_election* attribute is also true, the backup job will only be
run on the master host for the service.

`owner`: For filesystem paths, the user that will own the files upon
restore.

### Volumes

Volumes represent LVs that are associated with a service. They are
managed by float, which makes sure that they are present on the
hosts. Volumes aren't currently being ever removed, because we're
scared of deleting data.

Volumes are listed under the service attribute `volumes`, a list of
objects with the following attributes:

`name`: Volume name (mandatory).

`path`: Path where it should be mounted (mandatory).

`owner`: Owner of the mountpoint (default: root).

`group`: Group of the mountpoint (default: root).

`mode`: Mountpoint mode (default: 0755).

The LVs are created in the volume specified by the `volumes_vg` global
configuration variable, which by default is *vg0*. The VG must already
exist, float will not attempt to create it.

## Examples

Let's look at some example *services.yml* files:

```yaml
myservice:
  num_instances: 2
  service_credentials:
    - name: myservice
      enable_client: false
  public_endpoints:
    - name: myservice
      type: static
      port: 1234
```

This defines an Ansible-based service, of which we'll run two
instances. The service exposes an HTTP server on port 1234, which,
assuming *domain_public* is set to `mydomain.com`, will be available
at https://myservice.mydomain.com/ on the nginx service gateways of
the *core* role. Communication between the HTTPS gateway and the
service goes over HTTPS internally using auto-generated credentials.

```yaml
myservice:
  containers:
    - name: myapp
      image: myapp:latest
      port: 1234
    - name: redis
      image: redis
      port: 6379
  public_endpoints:
    - name: myservice
      type: static
      port: 1234
      scheme: http
```

The above describes a container-based service, consisting of two
separate processes: a web application, and a local Redis server (maybe
for caching). The two processes will always be scheduled together, so
*myapp* can be configured to talk to Redis on localhost:6379. This
time, the service gateway will talk to *myapp* over HTTP.

This service does not have any service credentials, but if it did they
would be bind-mounted under */etc/credentials* inside the container.

# Application credentials (*passwords.yml*)

The configuration for application-level credentials describes how to
generate those credentials. The location for this file cannot be
configured, it needs to be called *passwords.yml* and be stored in
your *credentials_dir*. Generated credentials will be encrypted with
ansible-vault and stored in a file called *secrets.yml* in that same
directory.

The credentials configuration file must contain a list of
dictionaries, each describing a separate credential. Supported
attributes include:

* `name` is the name of the Ansible variable that will be created in
  the resulting YAML file (mandatory)
* `description` is a human-readable description of what the credential
  represents
* `type` can be one of either *password* (the default) or *binary*,
  and it controls the character set of the resulting password. Right
  now, due to the requirement of command-line friendliness (it appears
  that Ansible is unable to correctly encode arbitrary strings when
  generating remote commands), both use the base64 charset.
* `length` is the desired length of the output.

A list element can, alternatively, contain a single `include`
attribute, in which case the contents of that file will be added to
the credentials list.

At the bare minimum, your *passwords.yml* file should include the
*passwords.yml.default* file from the float source tree, which
contains all the credentials for the built-in services:

```yaml
---
- include: "/path/to/float/passwords.yml.default"
```

# Global configuration variables

In order to customize the final environment for *your* installation,
there are some global configuration variables that you should
set. These are standard Ansible variables, and Ansible supports a few
ways to define them: one way would be to add them to the *all* group
in the inventory itself, but more conveniently, they can just be
placed in a YAML file *group_vars/all* somewhere next to it.

These variables are:

`domain` is the *internal* domain name used for hosts and internal
service resolution (see [service\_mesh.md](service_mesh.md#naming) for
more details). It is strongly suggested to use a dedicated domain for
this purpose, so it should be different from any public domain you
expect to be using (but it can be a subdomain of it, for instance
*internal.example.com*). With the current implementation, there's no
need for this domain to publicly exist at all, as name resolution is
done via /etc/hosts, but this may change in the future.

`domain_public` should contain a list of the public domain names that
will be used by the global HTTP router to export public HTTP service
endpoints. This is useful when public services should be reachable
equally on separate indipendent domains, like a primary one and a Tor
Hidden Service. If specifying multiple names, put the default public
one first - the first element of this list is used whenever we need a
human-readable identifier.

`testing` is a boolean variable, True by default, that indicates
whether we are running on a test or production environment. In test
environments, we load test data on services and databases.

`alert_email` is an email address that should receive alerts from the
monitoring system (optional).

## Network overlays

The `net_overlays` configuration variable should contain the list of
configured [network overlays](service_mesh.md#network-overlays), each
item a dictionary with the following attributes:

* `name` - name of the overlay network
* `port` (optional) - port used by the transport layer
* more transport-specific parameters are available, for the exact
  details see the [documentation of the net-overlay Ansible
  role](../roles/net-overlay/README.md#overlay-configuration).

## Admin users

The `admins` configuration variable contains a list of admin users,
each a dictionary with the following attributes:

* `name` - the username
* `email` (optional) - user's email address
* `password` - encrypted password. For a list of supported algorithms,
  check the [id/auth
  documentation](https://git.autistici.org/id/auth/blob/master/README.md#password-encoding).
* `totp_secret` - TOTP secret for 2FA, base32-encoded
* `ssh_keys` - a list of strings representing SSH public keys
* `u2f_registrations` - a list of objects representing U2F token
  registrations (see below for details)

### U2F Registrations

Hardware tokens supporting U2F can be registered for admin
users. Registration objects can be generated using the *pamu2fcfg*
command-line tool (part of the *pamu2fcfg* Debian package). The
following snippet should produce the two YAML attributes you need to
set (replace *login.example.com* with your SSO domain):

```shell
$ pamu2fcfg --nouser --appid https://login.example.com \
    | tr -d : \
    | awk -F, '{print "key_handle: \"" $1 "\"\npublic_key: \"" $2 "\""}'
```

Touch the hardware token to allow the above command to complete.

## SSH

If the `enable_ssh` configuration variable is true (the default),
*float* will take over the SSH configuration of the hosts, and perform
the following tasks:

* create a SSH Certification Authority in your *credentials_dir*
* sign the SSH host keys of all hosts with that CA
* add all the admin users *ssh_keys* to the authorized_key list for
  the **root** user on all hosts.

The underlying access model is very simple and expects admins to log
in as root in order to run Ansible, so you'll most likely want to set
*ansible_user=root* and *ansible_become=false* in your config as well.

Keys used for login will be logged in the audit log, so you can still
tell admins apart.

When `enable_ssh` is false, no changes whatsoever are made to the SSH
configuration of the hosts.

## Backups

To configure the backup system, you're going to need credentials for
an external repository. The backup system
uses [restic](https://restic.net), so check its documentation for the
URI syntax:

* `backup_repository_uri` - URI of the global (shared) restic repository
* `backup_repository_restic_password` - the password used to encrypt
  the restic repository.

